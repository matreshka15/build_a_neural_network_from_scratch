{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: This is a visualized version of neural networks with multiple layers lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from testCases_v4a import *\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#This is a implementation and library for binary classification, a.k.a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load iris dataset for debugging\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_breast_cancer()\n",
    "#print(dataset)\n",
    "inputx = dataset['data']\n",
    "print('Size of dataset:',inputx.shape)\n",
    "inputx = inputx.T\n",
    "print('Size of dataset:',inputx.shape)\n",
    "y = dataset['target']\n",
    "y = y.reshape(1,-1)\n",
    "print(y.shape)\n",
    "#print(dataset['data'])    #input value of dataset\n",
    "#print(dataset['target'])   #True label of iris datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "In order to implement neural networks, which turns out that we are using multiple computing units(e.g. sigmoid or relu), \n",
    "it is required to initialize the parameter w randomly, whileb can be set to all zeros.\n",
    "'''\n",
    "\n",
    "# nn_structure is a tuple, indicating the number of layers and units in each layer.\n",
    "# for example, nn_structure = [4,4,3,1] indicates a neural network with 1 input layer, 2 hidden layers and one output unit.\n",
    "def nn_parameter_initialize(nn_structure):\n",
    "    # x_dim indicates the dimensions of input feature,a bias unit b is defaultly set.\n",
    "    parameter = {}\n",
    "    for i in range(1,len(nn_structure)):\n",
    "        parameter['w'+str(i)] = np.random.randn(nn_structure[i],nn_structure[i-1])*0.01 \n",
    "        parameter['b'+str(i)] = np.zeros((nn_structure[i],1))\n",
    "    return parameter\n",
    "\n",
    "# Let's check whether the function is working properly or not\n",
    "test = nn_parameter_initialize([4,3,2,1])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of sigmoid function\n",
    "def sigmoid(z):\n",
    "    value = 1/(1+np.exp(-z))\n",
    "    return value\n",
    "\n",
    "def relu(z):\n",
    "    value = np.maximum(0,z)\n",
    "    return value\n",
    "\n",
    "def derivative_of_activation(a,activation='sigmoid'):\n",
    "    derivative = 0\n",
    "    shape = a.shape\n",
    "    if(activation == 'sigmoid'):\n",
    "        derivative = sigmoid(a)*(1-sigmoid(a))\n",
    "    elif(activation == 'relu'):\n",
    "        derivative = a>0\n",
    "        derivative = derivative.astype(np.int32)\n",
    "    return derivative.reshape(shape)\n",
    "\n",
    "#plot functions' shape\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Sigmoid function\\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,sigmoid(z))\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('ReLu function\\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,relu(z))\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Sigmoid function\\'s derivative \\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,derivative_of_activation(z))\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('ReLu\\'s derivative \\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,derivative_of_activation(z,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the dimensions of vectors:\n",
    "# w - (number_of_units_in_current_layer,number_of_units_in_previous_layer)\n",
    "# b - (number_of_units_in_current_layer,1) *broadcasted*\n",
    "# X - (x_dims,m)                           *m=number of samples\n",
    "\n",
    "'''\n",
    "Note that in neural networks with multiple layers, when doing forward prop, we need to cache some values for back prop step.\n",
    "For example:\n",
    "    when doing forward prop, we calculate z = np.dot(w,X)+b ,and then output a = g(z), when function g can be relu or sigmoid.\n",
    "    when doing back prop, we calculate dz = da*g'(z), where g'(z) is the derivative of g(z)\n",
    "    Therefore we can just cache z to save some time for calculating z multiple times.\n",
    "    \n",
    "In a nutshell, what we can cache in forward prop are: z,a,w\n",
    "'''\n",
    "\n",
    "# Forward propagation step: compute the predicted y's label\n",
    "def linear_forward_prop(w,b,X):\n",
    "    z = np.dot(w,X) + b\n",
    "    linear_cache = w\n",
    "    #print('Linear_forward_z=',z)\n",
    "    return z,linear_cache\n",
    "\n",
    "def single_layer_forward_prop(z,activation='relu'):\n",
    "    if(activation == 'relu'):\n",
    "        a = relu(z)\n",
    "    elif(activation == 'sigmoid'):\n",
    "        a = sigmoid(z)\n",
    "    activation_cache = a,z\n",
    "    return a,activation_cache\n",
    "\n",
    "def L_layer_forward_prop(X,parameters):\n",
    "    A = {0:X}\n",
    "    cache = {}  \n",
    "    L = len(parameters) // 2\n",
    "    for i in range(L-1):\n",
    "        w = parameters['w'+str(i+1)]\n",
    "        b = parameters['b'+str(i+1)]\n",
    "        z,linear_cache = linear_forward_prop(w,b,A[i])\n",
    "        a,activation_cache = single_layer_forward_prop(z,activation='relu')\n",
    "        A[i+1] = a\n",
    "        cache['layer_'+str(i+1)] = linear_cache,activation_cache\n",
    "    w = parameters['w'+str(L)]\n",
    "    b = parameters['b'+str(L)]    \n",
    "    z,linear_cache = linear_forward_prop(w,b,A[L-1])\n",
    "    yhat,activation_cache = single_layer_forward_prop(z,activation='sigmoid')\n",
    "    cache['layer_'+str(L)] = linear_cache,activation_cache\n",
    "    return yhat,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This part is used for testing robustness and debugging\n",
    "You can skip this part.\n",
    "the original testCases_v4a.py is programmed by professor Ng in his deeplearning courses.\n",
    "'''\n",
    "\n",
    "print('Single layer forward propagation test:')\n",
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "z,linear_cache = linear_forward_prop(W,b,A_prev)\n",
    "a,activation_cache = single_layer_forward_prop(z,activation='relu')\n",
    "print(\"With relu: A = \" + str(a))\n",
    "a,activation_cache = single_layer_forward_prop(z,activation='sigmoid')\n",
    "print(\"With sigmoid: A = \" + str(a))\n",
    "parameters = {}\n",
    "X, parameter = L_model_forward_test_case_2hidden()\n",
    "parameters['w1'] = parameter['W1']\n",
    "parameters['w2'] = parameter['W2']\n",
    "parameters['w3'] = parameter['W3']\n",
    "parameters['b3'] = parameter['b3']\n",
    "parameters['b2'] = parameter['b2']\n",
    "parameters['b1'] = parameter['b1']\n",
    "yhat,cache_from_forward = L_layer_forward_prop(X,parameters)\n",
    "print(yhat)\n",
    "L = len(cache_from_forward)\n",
    "W = {}\n",
    "A = {}\n",
    "Z = {}\n",
    "for layer in range(1,L+1):\n",
    "    linear_cache_l,activation_cache_l = cache_from_forward['layer_'+str(layer)]\n",
    "    W[layer] = linear_cache_l\n",
    "    A[layer],Z[layer] = activation_cache_l\n",
    "print(A[L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost function: used to check convergence\n",
    "def compute_cost(yhat,y):\n",
    "    m = yhat.shape[1]\n",
    "    cost = -np.sum(y*np.log(yhat)+(1-y)*np.log(1-yhat))/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagation step: compute partial derivatives of each parameter respectively\n",
    "def linear_back_prop(w,a_previous,dz_l):\n",
    "    m = a_previous.shape[1]\n",
    "    dw_l = np.dot(dz_l,a_previous.T)/m\n",
    "    assert(dw_l.shape == w.shape)\n",
    "    \n",
    "    db = np.sum(dz_l,axis=1,keepdims=True)/m\n",
    "    da_previous = np.dot(w.T,dz_l)\n",
    "    assert(da_previous.shape == a_previous.shape)\n",
    "    return dw_l,db,da_previous\n",
    " # Note: dw should have the same dimension as w have.Therefore back_prop returns dw.T\n",
    "    \n",
    "def single_layer_back_prop(da_l,z_l,activation):\n",
    "    derivative = derivative_of_activation(z_l,activation)\n",
    "    dz_l = da_l * derivative\n",
    "    assert(dz_l.shape == z_l.shape)\n",
    "    return dz_l\n",
    "\n",
    "def L_layer_back_prop(X,y,cache_from_forward,testAL = np.zeros(1)):\n",
    "    dW = {}\n",
    "    db = {}\n",
    "    dA = {}\n",
    "    W = {}\n",
    "    A = {0:X}\n",
    "    Z = {}\n",
    "    m = y.shape[1]\n",
    "    L=len(cache_from_forward)\n",
    "    for layer in range(1,L+1):\n",
    "        linear_cache_l,activation_cache_l = cache_from_forward['layer_'+str(layer)]\n",
    "        W[layer] = linear_cache_l\n",
    "        A[layer],Z[layer] = activation_cache_l\n",
    "    #print('AL = ',A[L])\n",
    "    #Initialize the output layer\n",
    "    if((testAL == np.zeros(1))):\n",
    "        yhat = A[L]\n",
    "    else:\n",
    "        yhat = testAL\n",
    "    dA[L] = -np.divide(y,yhat)+np.divide((1-y),(1-yhat))\n",
    "    dz_L = single_layer_back_prop(dA[L],Z[L],activation='sigmoid')\n",
    "    dW[L],db[L],dA[L-1] = linear_back_prop(W[L],A[L-1],dz_L)\n",
    "    \n",
    "    for i in reversed(range(1,L)):\n",
    "        dz_l = single_layer_back_prop(dA[i],Z[i],activation='relu')\n",
    "        dW[i],db[i],dA[i-1] = linear_back_prop(W[i],A[i-1],dz_l)\n",
    "        assert(dW[i].shape == W[i].shape)\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This part is used for testing robustness and debugging\n",
    "You can skip this part.\n",
    "the original testCases_v4a.py is programmed by professor Ng in his deeplearning courses.\n",
    "'''\n",
    "\n",
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "a_previous, w, b = linear_cache\n",
    "dW, db,dA_prev = linear_back_prop(w,a_previous,dZ) \n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))\n",
    "\n",
    "print('\\nSingle layer forward prop test:')\n",
    "da_l, linear_activation_cache = linear_activation_backward_test_case()\n",
    "linear_cache, activation_cache = linear_activation_cache\n",
    "a_previous, w, b = linear_cache\n",
    "z_l = activation_cache\n",
    "print()\n",
    "dz_l = single_layer_back_prop(da_l,z_l,activation='sigmoid')\n",
    "dW, db,dA_prev = linear_back_prop(w,a_previous,dz_l) \n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "dz_l = single_layer_back_prop(da_l,z_l,activation='relu')\n",
    "dW, db,dA_prev = linear_back_prop(w,a_previous,dz_l) \n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "cache_from_forward = {}\n",
    "print('Multiple layers back prop test:')\n",
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "print('L = ',len(caches))\n",
    "cache_2 = caches[1]\n",
    "cache_1 = caches[0]\n",
    "\n",
    "linear_cache, activation_cache = cache_2\n",
    "print(activation_cache.shape)\n",
    "a_previous, w, b = linear_cache\n",
    "print('w2.shape',w.shape)\n",
    "z_l = activation_cache\n",
    "linear_cache = w\n",
    "activation_cache = AL,z_l #a_previous,z_l\n",
    "cache_from_forward['layer_'+str(2)] = linear_cache,activation_cache\n",
    "linear_cache, activation_cache = cache_1\n",
    "X, w, b = linear_cache\n",
    "print('w1.shape',w.shape)\n",
    "z_l = activation_cache\n",
    "linear_cache = w\n",
    "activation_cache = a_previous,z_l\n",
    "cache_from_forward['layer_'+str(1)] = linear_cache,activation_cache\n",
    "print('X:',a_previous.shape)\n",
    "#dW,db = L_layer_back_prop(X,Y_assess,cache_from_forward,testAL = AL)\n",
    "print('dW = ',dW)\n",
    "print('db = ',db)\n",
    "cache_from_forward = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params,learning_rate,dW,dB):\n",
    "        L = len(params) // 2\n",
    "        for j in range(1,L+1):\n",
    "            params['w'+str(j)] = params['w'+str(j)] - learning_rate*dW[j]\n",
    "            params['b'+str(j)] = params['b'+str(j)] - learning_rate*dB[j]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This part is used for testing robustness and debugging\n",
    "You can skip this part.\n",
    "the original testCases_v4a.py is programmed by professor Ng in his deeplearning courses.\n",
    "'''\n",
    "\n",
    "dW = {}\n",
    "dB = {}\n",
    "params = {}\n",
    "parameters, grads = update_parameters_test_case()\n",
    "params['w1'] = parameters['W1']\n",
    "params['w2'] = parameters['W2']\n",
    "params['b1'] = parameters['b1']\n",
    "params['b2'] = parameters['b2']\n",
    "dW[1] = grads['dW1']\n",
    "dW[2] = grads['dW2']\n",
    "dB[1] = grads['db1']\n",
    "dB[2] = grads['db2']\n",
    "print(params)\n",
    "params = update_parameters(params,0.1,dW,dB)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall implementation of training a logistic regression\n",
    "# Note: net_structure indicates the shape of hidden layers and output layers. No input layer should be included.\n",
    "\n",
    "def train_neural_network(X,y,net_structure,number_of_iteration = 1000,learning_rate = 0.03,print_cost = True,plot_cost = True):\n",
    "    # Dimension convert: make sure all vectors are in proper shapes.\n",
    "    y = y.reshape(1,-1)# y is a row vector\n",
    "    \n",
    "    m = y.shape[1]  #  m = total number of trainning examples\n",
    "    if(X.shape[1] != m):\n",
    "        X=X.T       #=====> Note that array.reshape and array.T are different!\n",
    "    assert(X.shape[1] == m)\n",
    "    print('*******Dimension Check*******')\n",
    "    print('Input feature\\'s dimension: ',X.shape)\n",
    "    print('Output\\'s dimension: ',y.shape)\n",
    "    print('*****************************')\n",
    "    x_dim = X.shape[0]\n",
    "    L = len(net_structure) # number of layers\n",
    "    # Initialize parameters\n",
    "    nn_structure = [x_dim]+net_structure\n",
    "    params = nn_parameter_initialize(nn_structure)\n",
    "    print('Training {} layers neural network...'.format(len(params)//2))\n",
    "    if(plot_cost == True):\n",
    "        i_curve = []\n",
    "        cost_curve = []\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.title('Cross entropy')\n",
    "    \n",
    "    cache={}\n",
    "    for i in range(1,number_of_iteration+1):\n",
    "            # Steps:\n",
    "                # 1:Forward propagation\n",
    "                # 2:Compute cost\n",
    "                # 3:Backward Propagation\n",
    "                # 4:Update parameters\n",
    "                \n",
    "        yhat,cache = L_layer_forward_prop(X,params)\n",
    "        assert(yhat.shape == y.shape)\n",
    "        cost = compute_cost(yhat,y)\n",
    "        #print('yhat=',yhat)\n",
    "        dW,dB = L_layer_back_prop(X,y,cache)\n",
    "        #if(i%10 == 0):\n",
    "            #print('Iteration {},dW[2] = {}'.format(i,dW[2]))\n",
    "            \n",
    "        #Gradient decent\n",
    "        params = update_parameters(params,learning_rate,dW,dB)\n",
    "            \n",
    "        # Visualize the process of regression\n",
    "        if(i%100 == 0 and print_cost == True):\n",
    "            print('number of iterations:{}, cost = {}'.format(i,cost))\n",
    "        if(i%100 == 0 and plot_cost == True):\n",
    "            i_curve.append(i)\n",
    "            cost_curve.append(cost)\n",
    "            \n",
    "    if(plot_cost==True):        \n",
    "        i_curve = np.reshape(i_curve,(1,-1))\n",
    "        cost_curve = np.reshape(cost_curve,(1,-1))\n",
    "        plt.scatter(i_curve,cost_curve)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After training the unit, we can now use it to make predictions.\n",
    "def nn_predict(parameters,X,y=0,evaluate = True):\n",
    "    L = len(parameters) // 2\n",
    "    w1 = parameters['w1'] \n",
    "    num_of_features = w1.shape[1]\n",
    "#    m = y.shape[1]  #  m = total number of trainning examples\n",
    "    if(X.shape[0] != num_of_features):\n",
    "        X=X.T       #=====> Note that array.reshape and array.T are different!\n",
    "    yhat,cache = L_layer_forward_prop(X,parameters)\n",
    "    yhat = yhat>0.5\n",
    "    #Codes below is used to evaluate the performance of logistic regression on given dataset X with label y\n",
    "    #You can just ignore this part\n",
    "    if(evaluate == True):\n",
    "        y=y.reshape(1,-1)\n",
    "        train_accuracy = np.sum(yhat==y)/y.shape[1]\n",
    "        print('accuracy = %.2f\\n'%train_accuracy)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Goal:Wanna classify whether our patient's breast cancer is {}(0) or {}(1)\".format(dataset.target_names[0],dataset.target_names[1]))\n",
    "y = dataset['target']\n",
    "#Normalize input feature X\n",
    "X = dataset['data']\n",
    "X_norm = np.linalg.norm(X,axis=1,keepdims=True)\n",
    "X = (10*X)/ (np.max(X)-np.min(X))\n",
    "#Split up dataset in order to train as well as test the model\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the logistic unit\n",
    "print('Maximum of dataset: ',np.max(X_train))\n",
    "print('Minimum of dataset: ',np.min(X_train))\n",
    "print('Mean of dataset: ',np.mean(X_train))\n",
    "parameters = train_neural_network(X_train,y_train,[3,10,1],number_of_iteration = 20000,learning_rate = 0.1,print_cost = False,plot_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the unit on training set and test set\n",
    "print('Training accuracy:')\n",
    "Yhat = nn_predict(parameters,X_train,y_train,evaluate = True)\n",
    "print('Accuracy in test sets:')\n",
    "Ypredict = nn_predict(parameters,X_test,y_test,evaluate = True)\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=0.03,hidden_layer_sizes=(3,8), random_state=1)\n",
    "mlp = mlp.fit(X_train,y_train)\n",
    "train_score=mlp.score(X_train,y_train)#How many samples can the model predict right? \n",
    "print('sklearn\\'s neural network training accuracy:')\n",
    "print('%.2f'%train_score)\n",
    "#Okay, we have built our own neural network. Let's compare our unit with sklearn's! \n",
    "test_score=mlp.score(X_test,y_test)\n",
    "print('sklearn\\'s neural network training accuracy:')\n",
    "print('%.2f'%test_score)\n",
    "print('Loss = ',mlp.loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
