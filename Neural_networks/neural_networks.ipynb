{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: This is a visualized version of neural networks with multiple layers lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#This is a implementation and library for binary classification, a.k.a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load iris dataset for debugging\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_breast_cancer()\n",
    "#print(dataset)\n",
    "#print(dataset['data'])    #input value of dataset\n",
    "#print(dataset['target'])   #True label of iris datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "In order to implement neural networks, which turns out that we are using multiple computing units(e.g. sigmoid or relu), \n",
    "it is required to initialize the parameter w randomly, whileb can be set to all zeros.\n",
    "'''\n",
    "\n",
    "# nn_structure is a tuple, indicating the number of layers and units in each layer.\n",
    "# for example, nn_structure = [4,4,3,1] indicates a neural network with 1 input layer, 2 hidden layers and one output unit.\n",
    "def nn_parameter_initialize(nn_structure):\n",
    "    # x_dim indicates the dimensions of input feature,a bias unit b is defaultly set.\n",
    "    parameter = {}\n",
    "    for i in range(1,len(nn_structure)):\n",
    "        parameter['w'+str(i)] = np.random.randn(nn_structure[i],nn_structure[i-1])*0.01 \n",
    "        parameter['b'+str(i)] = np.zeros((nn_structure[i],1))\n",
    "    return parameter\n",
    "\n",
    "# Let's check whether the function is working properly or not\n",
    "test = nn_parameter_initialize([4,3,2,1])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of sigmoid function\n",
    "def sigmoid(z):\n",
    "    value = 1/(1+np.exp(-z))\n",
    "    return value\n",
    "\n",
    "def relu(z):\n",
    "    value = np.maximum(0,z)\n",
    "    return value\n",
    "\n",
    "def derivative_of_activation(a,activation='sigmoid'):\n",
    "    derivative = 0\n",
    "    shape = a.shape\n",
    "    if(activation == 'sigmoid'):\n",
    "        derivative = sigmoid(a)*(1-sigmoid(a))\n",
    "    elif(activation == 'relu'):\n",
    "        derivative = a>0\n",
    "        derivative = derivative.astype(np.int32)\n",
    "    return derivative.reshape(shape)\n",
    "\n",
    "#plot functions' shape\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Sigmoid function\\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,sigmoid(z))\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('ReLu function\\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,relu(z))\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Sigmoid function\\'s derivative \\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,derivative_of_activation(z))\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('ReLu\\'s derivative \\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,derivative_of_activation(z,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the dimensions of vectors:\n",
    "# w - (number_of_units_in_current_layer,number_of_units_in_previous_layer)\n",
    "# b - (number_of_units_in_current_layer,1) *broadcasted*\n",
    "# X - (x_dims,m)                           *m=number of samples\n",
    "\n",
    "'''\n",
    "Note that in neural networks with multiple layers, when doing forward prop, we need to cache some values for back prop step.\n",
    "For example:\n",
    "    when doing forward prop, we calculate z = np.dot(w,X)+b ,and then output a = g(z), when function g can be relu or sigmoid.\n",
    "    when doing back prop, we calculate dz = da*g'(z), where g'(z) is the derivative of g(z)\n",
    "    Therefore we can just cache z to save some time for calculating z multiple times.\n",
    "    \n",
    "In a nutshell, what we can cache in forward prop are: z,a,w\n",
    "'''\n",
    "\n",
    "# Forward propagation step: compute the predicted y's label\n",
    "def linear_forward_prop(w,b,X):\n",
    "    z = np.dot(w,X)+b\n",
    "    linear_cache = w\n",
    "    return z,linear_cache\n",
    "\n",
    "def single_layer_forward_prop(z,activation='relu'):\n",
    "    if(activation == 'relu'):\n",
    "        a = relu(z)\n",
    "    elif(activation == 'sigmoid'):\n",
    "        a = sigmoid(z)\n",
    "    activation_cache = a,z\n",
    "    return a,activation_cache\n",
    "\n",
    "def L_layer_forward_prop(X,parameters,L):\n",
    "    A = {0:X}\n",
    "    cache = {}  \n",
    "    for i in range(L-1):\n",
    "        w = parameters['w'+str(i+1)]\n",
    "        b = parameters['b'+str(i+1)]\n",
    "        z,linear_cache = linear_forward_prop(w,b,A[i])\n",
    "        a,activation_cache = single_layer_forward_prop(z,activation='relu')\n",
    "        A[i+1] = a\n",
    "        cache['layer_'+str(i+1)] = linear_cache,activation_cache\n",
    "    w = parameters['w'+str(L)]\n",
    "    b = parameters['b'+str(L)]    \n",
    "    z,linear_cache = linear_forward_prop(w,b,A[L-1])\n",
    "    yhat,activation_cache = single_layer_forward_prop(z,activation='sigmoid')\n",
    "    cache['layer_'+str(L)] = linear_cache,activation_cache\n",
    "    return yhat,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost function: used to check convergence\n",
    "def compute_cost(yhat,y):\n",
    "    m = yhat.shape[1]\n",
    "    cost = -np.sum(y*np.log(yhat)+(1-y)*np.log(1-yhat))/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagation step: compute partial derivatives of each parameter respectively\n",
    "def linear_back_prop(m,w,a_previous,dz_l):\n",
    "    #print('w_shape: ',w.shape)\n",
    "    #print('a dimension: ',a_previous.shape)\n",
    "    #print('dz\\'s shape: ',dz_l.shape)\n",
    "    dw_l = np.dot(a_previous,dz_l.T).T/m\n",
    "    #print('dw\\'s dimension: ',dw_l.shape)\n",
    "    #Make sure the dimensions corresponds.\n",
    "    assert(dw_l.shape == w.shape)\n",
    "    \n",
    "    db = np.sum(dz_l,axis=1,keepdims=True)/m\n",
    "    da_previous = np.dot(w.T,dz_l)\n",
    "    assert(da_previous.shape == a_previous.shape)\n",
    "    return dw_l,db,da_previous\n",
    " # Note: dw should have the same dimension as w have.Therefore back_prop returns dw.T\n",
    "    \n",
    "def single_layer_back_prop(da_l,z_l,activation):\n",
    "    derivative = derivative_of_activation(z_l,activation)\n",
    "    dz_l = da_l * derivative\n",
    "    assert(dz_l.shape == z_l.shape)\n",
    "    return dz_l\n",
    "\n",
    "def L_layer_back_prop(m,X,y,cache_from_forward):\n",
    "    dW = {}\n",
    "    db = {}\n",
    "    dA = {}\n",
    "    W = {}\n",
    "    A = {0:X}\n",
    "    Z = {}\n",
    "    L=len(cache_from_forward)\n",
    "    \n",
    "    for layer in range(1,L+1):\n",
    "        linear_cache_l,activation_cache_l = cache_from_forward['layer_'+str(layer)]\n",
    "        W[layer] = linear_cache_l\n",
    "        A[layer],Z[layer] = activation_cache_l\n",
    "    #Initialize the output layer\n",
    "    yhat = A[L]\n",
    "    #print(yhat)\n",
    "    dA[L] = -np.divide(y,yhat)+np.divide((1-y),(1-yhat))\n",
    "    dz_L = single_layer_back_prop(dA[L],Z[L],activation='sigmoid')\n",
    "    dW[L],db[L],dA[L-1] = linear_back_prop(m,W[L],A[L-1],dz_L)\n",
    "    for i in reversed(range(1,L)):\n",
    "        dz_l = single_layer_back_prop(dA[i],Z[i],activation='relu')\n",
    "        dW[i],db[i],dA[i-1] = linear_back_prop(m,W[i],A[i-1],dz_l)\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall implementation of trainning a logistic regression\n",
    "# Note: net_structure indicates the shape of hidden layers and output layers. No input layer should be included.\n",
    "\n",
    "def train_neural_network(X,y,net_structure,number_of_iteration = 1000,learning_rate = 0.03,print_cost = True,plot_cost = True):\n",
    "    # Dimension convert: make sure all vectors are in proper shapes.\n",
    "    y = y.reshape(1,-1)   # y is a row vector\n",
    "    m = y.shape[1]  #  m = total number of trainning examples\n",
    "    X = X.reshape(-1,m)\n",
    "    print('*******Dimension Check*******')\n",
    "    print('Input feature\\'s dimension: ',X.shape)\n",
    "    print('Output\\'s dimension: ',y.shape)\n",
    "    print('*****************************')\n",
    "    x_dim = X.shape[0]\n",
    "    L = len(net_structure) # number of layers\n",
    "    # Initialize parameters\n",
    "    nn_structure = [x_dim]+net_structure\n",
    "    params = nn_parameter_initialize(nn_structure)\n",
    "\n",
    "    if(plot_cost == True):\n",
    "        i_curve = []\n",
    "        cost_curve = []\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.title('Cross entrophy of regression')\n",
    "    \n",
    "    cache={}\n",
    "    for i in range(1,number_of_iteration+1):\n",
    "            # Steps:\n",
    "                # 1:Forward propagation\n",
    "                # 2:Compute cost\n",
    "                # 3:Backward Propagation\n",
    "                # 4:Update parameters\n",
    "                \n",
    "        yhat,cache = L_layer_forward_prop(X,params,L)\n",
    "        cost = compute_cost(yhat,y)\n",
    "        dW,dB = L_layer_back_prop(m,X,y,cache)\n",
    "        #print('Iteration {}'.format(i))    \n",
    "        #Gradient decent\n",
    "        for j in range(1,L+1):\n",
    "            params['w'+str(j)] = params['w'+str(j)] - learning_rate*dW[j]\n",
    "            params['b'+str(j)] = params['b'+str(j)] - learning_rate*dB[j]\n",
    "        \n",
    "        # Visualize the process of regression\n",
    "        if(i%100 == 0 and print_cost == True):\n",
    "            print('number of iterations:{}, cost = {}'.format(i,cost))\n",
    "        if(i%100 == 0 and plot_cost == True):\n",
    "            i_curve.append(i)\n",
    "            cost_curve.append(cost)\n",
    "            \n",
    "    if(plot_cost==True):        \n",
    "        i_curve = np.reshape(i_curve,(1,-1))\n",
    "        cost_curve = np.reshape(cost_curve,(1,-1))\n",
    "        plt.scatter(i_curve,cost_curve)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After training the unit, we can now use it to make predictions.\n",
    "def nn_predict(parameters,X,y=0,evaluate = True):\n",
    "    L = len(params) // 2\n",
    "    y = y.reshape(1,-1)   # y is a row vector\n",
    "    m = y.shape[1]  #  m = total number of trainning examples\n",
    "    X = X.reshape(-1,m)\n",
    "    yhat,cache = L_layer_forward_prop(X,parameters,L)\n",
    "    yhat = yhat>0.5\n",
    "    #Codes below is used to evaluate the performance of logistic regression on given dataset X with label y\n",
    "    #You can just ignore this part\n",
    "    if(evaluate == True):\n",
    "        y=y.reshape(1,-1)\n",
    "        train_accuracy = np.sum(yhat==y)/y.shape[1]\n",
    "        print('accuracy = %.2f\\n'%train_accuracy)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Goal:Wanna classify whether our patient's breast cancer is {}(0) or {}(1)\".format(dataset.target_names[0],dataset.target_names[1]))\n",
    "y = dataset['target']\n",
    "#Normalize input feature X\n",
    "X = dataset['data']\n",
    "X_norm = np.linalg.norm(X,axis=0,keepdims=True)\n",
    "X = (X-X_norm)/ (np.max(X)-np.min(X))\n",
    "#Split up dataset in order to train as well as test the model\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the logistic unit\n",
    "params = train_neural_network(X_train,y_train,[3,5,1],number_of_iteration = 100000,learning_rate = 0.1,print_cost = False,plot_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the unit on training set and test set\n",
    "print('Training accuracy:')\n",
    "Yhat = nn_predict(params,X_train,y_train,evaluate = True)\n",
    "print('Accuracy in test sets:')\n",
    "Ypredict = nn_predict(params,X_test,y_test,evaluate = True)\n",
    "\n",
    "'''\n",
    "#Okay, we have built our own logistic regression unit. Let's compare our unit with sklearn's! \n",
    "model=LogisticRegression(solver='liblinear')#Build a logistic regression model\n",
    "model=model.fit(X_train,y_train)#Train the model\n",
    "train_score=model.score(X_train,y_train)#How many samples can the model predict right? \n",
    "print('sklearn\\'s logistic regression training accuracy:')\n",
    "print('%.2f'%train_score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
