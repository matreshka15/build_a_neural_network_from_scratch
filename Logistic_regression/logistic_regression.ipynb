{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: This is a visualized version of logistic regression lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#This is a implementation and library for binary classification, a.k.a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load iris dataset for debugging\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_breast_cancer()\n",
    "#print(dataset)\n",
    "#print(dataset['data'])    #input value of dataset\n",
    "#print(dataset['target'])   #True label of iris datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "In order to implement logistic regression, we need to initialize parameters w and b, note that when using logistic regression, \n",
    "we have only one computing unit in the neural network. Therefore parameters can be initialized to all zeros. \n",
    "However, if we are using multiple computing units(e.g. sigmoid or relu), it is required to initialize the parameter w randomly, while\n",
    "b can be set to all zeros.\n",
    "'''\n",
    "def logistic_parameter_initialize(x_dim):\n",
    "    # x_dim indicates the dimensions of input feature,a bias unit b is defaultly set.\n",
    "    w = np.zeros((1,x_dim))\n",
    "    b = 0\n",
    "    \n",
    "    parameter = {'w':w,'b':b}\n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of sigmoid function\n",
    "def sigmoid(z):\n",
    "    value = 1/(1+np.exp(-z))\n",
    "    return value\n",
    "\n",
    "#plot Sigmoid function \n",
    "plt.figure(figsize=(4,3))\n",
    "plt.title('Sigmoid function\\'s shape')\n",
    "z = np.linspace(-10,10)\n",
    "plt.plot(z,sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note the dimensions of vectors:\n",
    "# w - (1,x_dims)\n",
    "# b - (1,x_dims) *broadcasted*\n",
    "# X - (x_dims,m) *m=number of samples\n",
    "\n",
    "\n",
    "# Forward propagation step: compute the predicted y's label\n",
    "def forward_prop(w,b,X):\n",
    "    z = np.dot(w,X)+b\n",
    "    a = sigmoid(z)\n",
    "    return z,a\n",
    "\n",
    "# Compute cost function: used to check convergence\n",
    "def compute_cost(a,y):\n",
    "    m = a.shape[1]\n",
    "    cost = -np.sum(y*np.log(a)+(1-y)*np.log(1-a))/m\n",
    "    return cost\n",
    "\n",
    "# Back propagation step: compute partial derivatives of each parameter respectively\n",
    "def back_prop(X,a,y):\n",
    "    m = a.shape[1]\n",
    "    dz = a - y\n",
    "    dw = np.dot(X,dz.T).T/m\n",
    "    db= np.sum(dz)/m\n",
    "    # Note: dw should have the same dimension as w have.Therefore back_prop return dw.T\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overall implementation of trainning a logistic regression\n",
    "def train_logistic_regression(X,y,number_of_iteration = 1000,learning_rate = 0.03,print_cost = True,plot_cost = True):\n",
    "    # Dimension convert: make sure all vectors are in proper shapes.\n",
    "    y = y.reshape(1,-1)   # y is a row vector\n",
    "    m = y.shape[1]  #  m = total number of trainning examples\n",
    "    X = X.reshape(-1,m)\n",
    "    x_dim = X.shape[0]\n",
    "    # Initialize parameters\n",
    "    params = logistic_parameter_initialize(x_dim)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    if(plot_cost == True):\n",
    "        i_curve = []\n",
    "        cost_curve = []\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.title('Cross entrophy of regression')\n",
    "    \n",
    "    for i in range(1,number_of_iteration+1):\n",
    "        z,a = forward_prop(w,b,X)\n",
    "        dw,db = back_prop(X,a,y)\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        cost = compute_cost(a,y)\n",
    "        # Visualize the process of regression\n",
    "        if(i%100 == 0 and print_cost == True):\n",
    "            print('number of iterations:{}, cost = {}'.format(i,cost))\n",
    "        if(i%100 == 0 and plot_cost == True):\n",
    "            i_curve.append(i)\n",
    "            cost_curve.append(cost)\n",
    "    if(plot_cost==True):        \n",
    "        i_curve = np.reshape(i_curve,(1,-1))\n",
    "        cost_curve = np.reshape(cost_curve,(1,-1))\n",
    "        plt.scatter(i_curve,cost_curve)\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After training the unit, we can now use it to make predictions.\n",
    "def logistic_predict(w,b,X,y=0,evaluate = True):\n",
    "    if(not w.shape[1] == X.shape[0]):\n",
    "        X = X.T\n",
    "    yhat = np.dot(w,X)+b\n",
    "    yhat = yhat>0.5\n",
    "    #Codes below is used to evaluate the performance of logistic regression on given dataset X with label y\n",
    "    #You can just ignore this part\n",
    "    if(evaluate == True):\n",
    "        y=y.reshape(1,-1)\n",
    "        train_accuracy = np.sum(yhat==y)/y.shape[1]\n",
    "        print('accuracy = %.2f\\n'%train_accuracy)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Goal:Wanna classify whether our patient's breast cancer is {}(0) or {}(1)\".format(dataset.target_names[0],dataset.target_names[1]))\n",
    "y = dataset['target']\n",
    "#Normalize input feature X\n",
    "X = dataset['data']\n",
    "X_norm = np.linalg.norm(X,axis=0,keepdims=True)\n",
    "X /= X_norm\n",
    "#Split up dataset in order to train as well as test the model\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic unit\n",
    "w,b = train_logistic_regression(X_train,y_train,number_of_iteration = 30000,learning_rate = 3,print_cost = False,plot_cost = True)\n",
    "\n",
    "# Evaluate the performance of the unit on training set and test set\n",
    "print('Training accuracy:')\n",
    "Yhat = logistic_predict(w,b,X_train,y_train,evaluate = True)\n",
    "print('Accuracy in test sets:')\n",
    "Ypredict = logistic_predict(w,b,X_test,y_test,evaluate = True)\n",
    "\n",
    "#Okay, we have built our own logistic regression unit. Let's compare our unit with sklearn's! \n",
    "model=LogisticRegression(solver='liblinear')#Build a logistic regression model\n",
    "model=model.fit(X_train,y_train)#Train the model\n",
    "train_score=model.score(X_train,y_train)#How many samples can the model predict right? \n",
    "print('sklearn\\'s logistic regression training accuracy:')\n",
    "print('%.2f'%train_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
